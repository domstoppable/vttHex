---
title: "Vibey Transcribey Results"
output: html_document
---

R code for processing vibey-transcribey data

```{r load-helpers, include = FALSE}
    # helper functions and folder setup

    # ensure output plots exist
    if (!file.exists("analysis-output/plots")) {
        dir.create("analysis-output/plots", recursive = TRUE)
    }

    # Load helper funcs and dataframes and set directory
    phone_translate <- read.csv("aux-data/phoneme-translate.csv")
    phone_translate$phone_type <- as.factor(phone_translate$phone_type)
    training_lexicon <- read.csv("aux-data/training-lexicon.csv")

    load_eval_files <- function(file_pattern, create_correct_column = TRUE) {
        # find matching files, combine into single df
        setwd("data/evals")
        df_files <- list.files(".", file_pattern)
        df <- dplyr::bind_rows(lapply(df_files, read.csv))
        setwd("../../")

        # Remove unneeded columns and rows
        df <- subset(df, select = -c(timestamp, facilitator, event, item))
        df <- dplyr::filter(df, stimulus != "")

        if (create_correct_column) {
            df$correct <- as.integer(df$stimulus == df$selection)
        }

        # configure data types
        ordered_conditions <- c("Pre-test", "Post-test")
        df <- within(df, {
            pid <- as.factor(pid)
            condition <- factor(
                condition,
                levels = ordered_conditions,
                labels = ordered_conditions
            )
            selection <- as.factor(selection)
            stimulus <- as.factor(stimulus)
        })

        return(df)
    }

    merge_eval_files_with_training <- function(eval_df, train_df) {
        df <- merge(eval_df, train_df, key = "pid", all.x = TRUE)
        df$training_word_exposures <- ifelse(
            df$condition == "Pre-test",
            0,
            df$training_word_exposures
        )

        return(df)
    }

    bin_lme <- function(formula, data) {
        data <- rlang::enexpr(data)
        model_call <- rlang::expr(lme4::glmer(
            formula = formula,
            data = !!data,
            family = binomial,
            control = lme4::glmerControl(optimizer = "bobyqa")
        ))

        model <- eval(model_call)
        print(summary(model))

        return(model)
    }

    save_last_plot <- function(filename, width = 6, height = 6) {
        ggplot2::ggsave(
            plot = ggplot2::last_plot(),
            file = paste("analysis-output/plots/", filename, sep = "/"),
            width = width, height = height
        )
    }
```

# TRAINING DATA
```{r load-training-data, echo = FALSE}
    # Load training files from data/training/*.csv

    setwd("data/training")

    training_files <- list.files(".", "*.csv")
    training_data <- data.frame()

    for (idx in seq_along(training_files)) {
        tmp_df <- read.csv(training_files[idx])
        if (nrow(tmp_df) > 0) {
            training_data <- dplyr::bind_rows(training_data, tmp_df)
        }
    }
    setwd("../../")

    # calculate how much time has elapsed between rows
    # from this we can determine training day index
    training_data <- training_data[
        order(training_data$PID, training_data$Timestamp),
    ]

    training_data$datetime <- as.POSIXct(sub("T", " ", training_data$Timestamp))
    # $timedelta will be in seconds
    is_repeated_pid <- training_data$PID == dplyr::lag(training_data$PID)
    training_data$timedelta <- ifelse(
        is_repeated_pid & !is.na(dplyr::lag(training_data$datetime)),
        training_data$datetime - dplyr::lag(training_data$datetime),
        Inf
    )

    # Compute training session idx's
    # we consider a gap of 1 hour or more to count as a new training session
    training_data$training_session_inc <- ifelse(
        training_data$timedelta == Inf,
        0,
        as.integer(training_data$timedelta > 3600)
    )
    training_data$training_session_idx <- cumsum(training_data$training_session_inc)

    tmp_first_day <- data.frame(dplyr::summarise(
        dplyr::group_by(training_data, PID),
        first_day = min(training_session_idx)
    ))

    training_data <- dplyr::left_join(training_data, tmp_first_day, c("PID"))
    training_data$training_session_idx <- training_data$training_session_idx - training_data$first_day

    # Compute level attempt idx's
    is_not_first_row <- is.na(lag(training_data$training_session_idx))
    is_repeated_pid <- training_data$PID != lag(training_data$PID)
    training_data$level_attempt_idx <- ifelse(
        is_not_first_row | is_repeated_pid | is.na(dplyr::lag(training_data$LevelAttemptGuid)),
        1,
        training_data$LevelAttemptGuid != dplyr::lag(training_data$LevelAttemptGuid)
    )
    training_data$level_attempt_idx <- cumsum(training_data$level_attempt_idx)

    # trunc file extension from stimulus column
    training_data$stimulus <- sub("\\..*", "", training_data$Stimulus)

    # parse speaker/phrase from stimulus filename
    stimuli_info <- data.frame(do.call("rbind", strsplit(training_data$stimulus, "-")))
    training_data$speaker <- stimuli_info$X1
    training_data$phrase <- stimuli_info$X2

    # configure data types
    training_data <- within(training_data, {
        pid <- as.factor(PID)
        stimulus <- as.factor(stimulus)
        speaker <- as.factor(speaker)
    })
    training_data <- merge(training_data, training_lexicon, by = "stimulus", all.x = TRUE)
    training_data <- training_data[order(training_data$pid, training_data$Timestamp), ]

    # clear unneeded columns
    training_data <- subset(training_data, select = c(
        Timestamp, datetime, pid, stimulus, Ok, speaker, phrase.y,
        training_session_idx, level_attempt_idx, LevelAttemptGuid
    ))
    training_data <- dplyr::rename(training_data, phrase = phrase.y)

    # Count exposures by VTT file
    exposure_counts_by_stimulus <- data.frame(dplyr::summarise(
        dplyr::group_by(training_data, pid, stimulus, phrase),
        stimulusExposures = sum(Ok)
    ))

    write.csv(exposure_counts_by_stimulus, "analysis-output/exposure_counts_by_stimulus.csv")

    # Count exposures by phrase (each phrase has 4 VTT files)
    exposure_counts_by_phrase <- data.frame(dplyr::summarise(
        dplyr::group_by(training_data, pid, phrase),
        phraseExposures = dplyr::n()
    ))

    write.csv(exposure_counts_by_phrase, "analysis-output/exposure_counts_by_phrase.csv")

    # Count how many total words each participant received
    traing_word_exposures <- data.frame(dplyr::summarise(
        dplyr::group_by(training_data, pid),
        training_word_exposures = sum(Ok)
    ))

    # Given how many of which stimuli each participant was exposed to,
    # calculate how many times each pid was exposed to each phoneme
    exposure_counts_by_phoneme <- merge(exposure_counts_by_stimulus, training_lexicon, by = "stimulus", all.x = TRUE)
    exposure_counts_by_phoneme <- dplyr::rename(exposure_counts_by_phoneme, phrase = phrase.y)

    # validity checking
    missing_pronounciations <- is.na(exposure_counts_by_phoneme$pronounciation)
    words_not_in_lexicon <- exposure_counts_by_phoneme[missing_pronounciations, ]
    if (nrow(words_not_in_lexicon) > 0) {
        print("### ASSERT FAILED: ###")
        print("Words not in the lexicon:")
        print(words_not_in_lexicon$word)
    }
    pronounciation_counts <- data.frame(dplyr::summarise(
        dplyr::group_by(training_lexicon, phrase),
        pronounciations = dplyr::n()
    ))
    pronounciation_counts <- dplyr::filter(pronounciation_counts, pronounciations != 4)
    if (nrow(pronounciation_counts) > 0) {
        print("### ASSERT FAILED: ###")
        print("Words with invalid pronounciation counts")
        print(pronounciation_counts)
    }

    exposure_counts_by_phoneme <- dplyr::mutate(exposure_counts_by_phoneme, across(AA:ZH, ~ .x * stimulusExposures))
    write.csv(exposure_counts_by_phoneme, "analysis-output/exposure_counts_by_phoneme_0.csv")

    p_exposures <- tidyr::gather(exposure_counts_by_phoneme, phoneme, phoneme_exposures, AA:ZH)
    p_exposures <- data.frame(dplyr::summarise(
        dplyr::group_by(p_exposures, pid, phoneme),
        phoneme_exposures = sum(phoneme_exposures)
    ))

    write.csv(p_exposures, "analysis-output/exposure_counts_by_phoneme.csv")

    # Cleanup
    rm(idx)
    rm(tmp_df)
    rm(training_files)
    rm(tmp_first_day)
    rm(stimuli_info)
    rm(exposure_counts_by_stimulus)
    rm(words_not_in_lexicon)
    rm(pronounciation_counts)
    rm(exposure_counts_by_phoneme)
    rm(is_repeated_pid)
    rm(is_not_first_row)
    rm(missing_pronounciations)
```

## Training Reports
```{r create-training-reports, echo = FALSE}
    training_day_summary <- data.frame(dplyr::summarise(
        dplyr::group_by(training_data, pid, training_session_idx),
        startTime = min(datetime),
        stopTime = max(datetime),
        phraseStimCount = sum(Ok),
        durationInM = 60 * as.double(difftime(max(datetime), min(datetime), units = "hours"))
    ))

    stims_per_level <- data.frame(dplyr::summarise(
        dplyr::group_by(training_data, pid, LevelAttemptGuid),
        levelStartTime = min(Timestamp),
        stimCount = sum(Ok)
    ))

    uniq_phrases <- data.frame(dplyr::summarise(
        dplyr::group_by(exposure_counts_by_phrase, pid),
        uniquePhrases = dplyr::n(),
        phrases = paste(phrase, collapse = "|")
    ))

    total_p_exposures_by_pid <- data.frame(dplyr::summarise(
        dplyr::group_by(p_exposures, pid),
        totalPhonemeExposures = sum(phoneme_exposures),
        meanPhonemeExposures = mean(phoneme_exposures),
        stdevPhonemeExposures = sd(phoneme_exposures)
    ))

    untrained_phones <- data.frame(dplyr::summarise(
        dplyr::group_by(dplyr::filter(p_exposures, phoneme_exposures == 0), pid),
        untrainedPhones = paste(phoneme, collapse = "|")
    ))

    training_report <- data.frame(dplyr::summarise(
        dplyr::group_by(training_data, pid),
        first_day = min(datetime),
        lastDay = max(datetime),
        uniqueSessions = max(training_session_idx),
        dateRange = as.integer(max(datetime) - min(datetime)),
        totalPhraseStimExposures = sum(Ok)
    ))

    training_day_summary_by_pid <- data.frame(dplyr::summarise(
        dplyr::group_by(training_day_summary, pid),
        meantrainingSessionDurationM = mean(durationInM)
    ))

    level_attempts_by_pid <- data.frame(dplyr::summarise(
        dplyr::group_by(stims_per_level, pid),
        levelAttempts = dplyr::n()
    ))

    training_report <- merge(training_report, level_attempts_by_pid, by = "pid", keep.x = TRUE)
    training_report <- merge(training_report, training_day_summary_by_pid, by = "pid", keep.x = TRUE)
    training_report$level_attempts_per_session <- training_report$levelAttempts / training_report$uniqueSessions
    training_report <- merge(training_report, total_p_exposures_by_pid, by = "pid", keep.x = TRUE)
    training_report <- merge(training_report, untrained_phones, by = "pid", keep.x = TRUE)
    training_report <- merge(training_report, uniq_phrases, by = "pid", keep.x = TRUE)

    write.csv(training_report, "analysis-output/training_report.csv")

    # Cleanup
    rm(training_day_summary)
    rm(stims_per_level)
    rm(uniq_phrases)
    rm(total_p_exposures_by_pid)
    rm(untrained_phones)
    rm(training_day_summary_by_pid)
    rm(level_attempts_by_pid)
```

# PHONEMES
## Load data
```{r load-phoneme-data, echo = FALSE}
    phone_df <- load_eval_files("_phonemes_")
    phone_df <- merge_eval_files_with_training(phone_df, traing_word_exposures)

    phone_df <- within(phone_df, {
        speaker_gender <- as.factor(toupper(substring(stimfile, 1, 1)))
        stimfile <- as.factor(stimfile)
    })

    # Manually check ratios
    summary(phone_df$correct)
    table(phone_df$pid, phone_df$correct, phone_df$condition)
    table(phone_df$pid, phone_df$correct)
    table(phone_df$condition, phone_df$correct)
```

## Explore and visualize ðŸ“ˆ
```{r plot-phoneme-data, echo = FALSE}
    phone_df_with_phone_exposures <- merge(
        phone_df, phone_translate,
        by.x = "stimulus", by.y = "test_word",
        keep.x = TRUE
    )

    phone_df_with_phone_exposures <- dplyr::left_join(
        phone_df_with_phone_exposures,
        p_exposures,
        c("train_phone" = "phoneme", "pid")
    )

    pre_test_only <- dplyr::filter(phone_df_with_phone_exposures, condition == "Pre-test")

    phone_df_with_phone_exposures$phoneme_exposures <- ifelse(
        phone_df_with_phone_exposures$condition == "Pre-test",
        0,
        phone_df_with_phone_exposures$phoneme_exposures
    )

    post_test_only <- dplyr::filter(phone_df_with_phone_exposures, condition == "Post-test")

    exposure_counts_by_phone_type <- data.frame(
        dplyr::summarise(
            dplyr::group_by(phone_df_with_phone_exposures, pid, condition, test_phone, phone_type),
            phoneme_exposures = mean(phoneme_exposures),
        )
    )

    exposure_counts_by_phone_type <- data.frame(
        dplyr::summarise(
            dplyr::group_by(exposure_counts_by_phone_type, pid, phone_type, condition),
            phoneme_exposures = sum(phoneme_exposures),
        )
    )

    test_summary <- data.frame(
        dplyr::summarise(
            dplyr::group_by(phone_df_with_phone_exposures, condition, pid, phone_type),
            correctPortion = 100 * mean(correct),
            se = 100 * sd(correct) / sqrt(length(correct))
        )
    )

    phone_summary_by_type_and_pid <- merge(
        test_summary,
        exposure_counts_by_phone_type,
        by = c("condition", "pid", "phone_type")
    )

    ggplot2::ggplot(phone_summary_by_type_and_pid, ggplot2::aes(
        x = phoneme_exposures, y = correctPortion,
        shape = phone_type, group = phone_type, color = phone_type
    )) +
        ggplot2::facet_wrap(facets = ggplot2::vars(pid)) +
        ggplot2::geom_hline(yintercept = 100 / 9, linetype = "dashed", color = "gray") +
        ggplot2::geom_line() +
        ggplot2::geom_point(size = 3) +
        ggplot2::geom_errorbar(
            ggplot2::aes(ymin = correctPortion - se, ymax = correctPortion + se),
            width = 500,
        ) +
        ggplot2::ylim(0, 100) +
        ggplot2::labs(
            colour = "Phoneme Type",
            shape = "Phoneme Type",
            x = "Phoneme Exposures",
            y = "Percentage Correct",
            title = "Phoneme Recognition (pre vs post-test)"
        ) +
        ggplot2::theme_classic(base_size = 10) +
        ggplot2::theme(legend.position = c(0.85, 0.85))

    save_last_plot("phoneme-recognition-by-pid.png")

    post_test_summary_by_type <- data.frame(
        dplyr::summarise(
            dplyr::group_by(phone_summary_by_type_and_pid, phone_type, condition),
            correctPortion = mean(correctPortion),
            phoneme_exposures = mean(phoneme_exposures),
            se = sd(correctPortion) / sqrt(length(correctPortion))
        )
    )

    ggplot2::ggplot(post_test_summary_by_type, ggplot2::aes(
        x = phoneme_exposures, y = correctPortion,
        shape = phone_type, group = phone_type, color = phone_type
    )) +
        ggplot2::geom_hline(yintercept = 100 / 9, linetype = "dashed", color = "gray") +
        ggplot2::geom_line() +
        ggplot2::geom_point(size = 3) +
        ggplot2::geom_errorbar(
            ggplot2::aes(ymin = correctPortion - se, ymax = correctPortion + se),
            width = 500,
        ) +
        ggplot2::ylim(0, 100) +
        ggplot2::labs(
            colour = "Phoneme Type",
            shape = "Phoneme Type",
    #        x = "Phoneme Exposures"
    #       y = "Percentage Correct",
            title = "All Participants"
        ) +
        ggplot2::theme_classic(base_size = 10) +
        ggplot2::theme(
            legend.position = "none",
            axis.title.x = ggplot2::element_blank(), axis.title.y = ggplot2::element_blank()
        )
    #    ggplot2::theme(legend.position = c(0.85, 0.85))

    save_last_plot("phoneme-recognition.png", width = 3, height = 3)

    post_test_summary <- data.frame(
        dplyr::summarise(
            dplyr::group_by(post_test_only, phone_type, test_phone),
            correctPortion = 100 * mean(correct),
            phoneme_exposures = mean(phoneme_exposures),
            se = 100 * sd(correct) / sqrt(length(correct))
        )
    )

    ggplot2::ggplot(post_test_summary, ggplot2::aes(
        x = phoneme_exposures, y = correctPortion,
        label = test_phone, color = phone_type
    )) +
        ggplot2::geom_hline(yintercept = 100 / 9, linetype = "dashed", color = "gray") +
    #    ggplot2::geom_line(ggplot2::aes(y = fitLine$fit), color = 'black') +
    #    ggplot2::geom_ribbon(ggplot2::aes(ymin = fitLine$lwr, ymax = fitLine$upr), alpha = .15, color = 'gray') +
    #    ggplot2::geom_point(show.legend = FALSE) +
    #    ggrepel::geom_text_repel(max.overlaps = Inf) +
        ggplot2::geom_text(size = 3) +
        #ggplot2::ylim(min(fitLine$lwr), 100) +
        ggplot2::ylim(0, 100) +
        ggplot2::labs(
            colour = "Phoneme Type",
    #        x = "Phoneme Exposures",
    #        y = "Percentage Correct",
            title = "All Participants"
        ) +
        ggplot2::theme_classic(base_size = 10) +
        ggplot2::theme(
            legend.position = "none",
            axis.title.x = ggplot2::element_blank(), axis.title.y = ggplot2::element_blank()
        )

    save_last_plot("phoneme-recognition-by-phone.png", width = 3, height = 3)

    post_test_summary_by_pid <- data.frame(
        dplyr::summarise(
            dplyr::group_by(post_test_only, pid, phone_type, test_phone),
            correctPortion = 100 * mean(correct),
            phoneme_exposures = mean(phoneme_exposures),
            se = 100 * sd(correct) / sqrt(length(correct))
        )
    )

    ggplot2::ggplot(post_test_summary_by_pid, ggplot2::aes(
        x = phoneme_exposures, y = correctPortion,
        label = test_phone, color = phone_type
    )) +
        ggplot2::facet_wrap(facets = ggplot2::vars(pid)) +
        ggplot2::geom_hline(yintercept = 100 / 9, linetype = "dashed", color = "gray") +
    #    ggplot2::geom_line(ggplot2::aes(y = fitLine$fit), color = 'black') +
    #    ggplot2::geom_ribbon(ggplot2::aes(ymin = fitLine$lwr, ymax = fitLine$upr), alpha = .15, color = 'gray') +
    #    ggplot2::geom_point(show.legend = FALSE) +
        ggplot2::geom_text(size = 3) +
    #    ggrepel::geom_text_repel(max.overlaps = Inf) +
        #ggplot2::ylim(min(fitLine$lwr), 100) +
        ggplot2::ylim(0, 100) +
        ggplot2::labs(
            colour = "Phoneme Type",
            x = "Phoneme Exposures",
            y = "Percentage Correct",
            title = "Post-test Phoneme Identification"
        ) +
        ggplot2::theme_classic(base_size = 10) +
        ggplot2::theme(legend.position = c(0.85, 0.85))

    save_last_plot("phoneme-recognition-by-phone-by-pid.png")
```

## Model
```{r model-phoneme-data, echo = FALSE}
    phone_model <- function(formula) bin_lme(formula = formula, data = post_test_only)

    phone_m0 <- phone_model(correct ~ 1 + (phoneme_exposures | pid))
    phone_m1 <- phone_model(correct ~ phoneme_exposures + (phoneme_exposures | pid))
    phone_m2 <- phone_model(correct ~ phoneme_exposures + phone_type + (phoneme_exposures | pid))

    #plot(ggeffects::ggeffect(phone_m1, terms = c("training [0:50]"), type = "re"))
    #plot(ggeffects::ggeffect(phone_m2, terms = c("training [0:50]", "phone_type"), type = "re"))
    #plot(ggeffects::ggeffect(phone_m3, terms = c("training [0:50]", "phone_type"), type = "re"))

    anova(phone_m0, phone_m1, phone_m2)
```

# Prosody
## Load Data
```{r load-prosody-data, echo = FALSE}
    prosody_df <- load_eval_files("_prosody_")
    prosody_df <- merge_eval_files_with_training(prosody_df, traing_word_exposures)

    prosody_df <- tidyr::separate(
        data = prosody_df, col = stimfile,
        into = "sentence_id",
        sep = "_",
        extra = "drop",
        remove = FALSE
    )
    type_starts_with_f <- substring(prosody_df$stimfile, 1, 1) == "f"
    prosody_df$prosody_type <- as.factor(ifelse(type_starts_with_f, "Focus", "Phrase"))
    prosody_df$stimfile <- as.factor(prosody_df$stimfile)

    #prosody_df <- data.frame(dplyr::summarise(
    #    dplyr::group_by(prosody_df, pid, condition, sentence_id, prosody_type),
    #    training_word_exposures = mean(training_word_exposures),
    #    portionCorrect = mean(correct)
    #))

    focus_df <- dplyr::filter(prosody_df, prosody_df$prosody_type == "Focus")
    focus_df$sentence_id <- as.factor(focus_df$sentence_id)

    phrase_df <- dplyr::filter(prosody_df, prosody_df$prosody_type == "Phrase")
    phrase_df$sentence_id <- as.factor(phrase_df$sentence_id)

    # cleanup
    rm(type_starts_with_f)
```

## Plot
```{r plot-prosody-data, echo = FALSE}
    prosody_summary <- dplyr::summarise(
        dplyr::group_by(prosody_df, pid, prosody_type, condition),
        correctPortion = 100 * mean(correct),
        se = 100 * sd(correct) / sqrt(length(correct)),
        training_word_exposures = mean(training_word_exposures)
    )

    ggplot2::ggplot(prosody_summary, ggplot2::aes(
        x = training_word_exposures, y = correctPortion,
        shape = prosody_type,
        group = prosody_type,
        color = prosody_type
    )) +
        ggplot2::facet_wrap(facets = ggplot2::vars(pid)) +
        ggplot2::geom_hline(yintercept = 50, linetype = "dashed", color = "gray") +
        ggplot2::geom_line() +
        ggplot2::geom_point(size = 4) +
        ggplot2::geom_errorbar(
            ggplot2::aes(ymin = correctPortion - se, ymax = correctPortion + se),
            width = 250
        ) +
        ggplot2::ylim(0, 100) +
        ggplot2::labs(
            colour = "",
            shape = "",
            x = "Training Word Exposures",
            y = "Percentage Correct",
            title = "Prosody Perception"
        ) +
        ggplot2::theme_classic(base_size = 10) +
        ggplot2::theme(legend.position = c(0.5, 0.065), legend.direction = "horizontal")

    save_last_plot("prosody-perception.png")

```

## Focus - Model & Test
```{r model-focus-data, echo = FALSE}
    focus_model <- function(formula) bin_lme(formula = formula, data = focus_df)
    #
    focus_m0 <- focus_model(correct ~ 1 + (training_word_exposures | pid))
    focus_m1 <- focus_model(correct ~ training_word_exposures + (training_word_exposures | pid))
    focus_m2 <- focus_model(correct ~ training_word_exposures + (training_word_exposures | pid) + (1 | sentence_id))

    #focus_model <- function(formula) lme(formula = formula, data = focus_df)
    #lme4::lmer(portionCorrect ~ 1 + (training_word_exposures|pid), data = focus_df)
    #
    #focus_m0 = focus_model(correct ~ 1 + (training_word_exposures | pid))
    #focus_m1 = focus_model(correct ~ training_word_exposures + (training_word_exposures | pid))
    #focus_m2 = focus_model(correct ~ training_word_exposures + (training_word_exposures | pid) + (1 | sentence_id))


    anova(focus_m0, focus_m1, focus_m2)

```

## Phrase Boundary - Model & Test
```{r model-phrase-data, echo = FALSE}
    phrase_model <- function(formula) bin_lme(formula = formula, data = phrase_df)

    phrase_m0 <- phrase_model(correct ~ 1 + (training_word_exposures | pid))
    phrase_m1 <- phrase_model(correct ~ training_word_exposures + (training_word_exposures | pid))
    phrase_m2 <- phrase_model(correct ~ training_word_exposures + (training_word_exposures | pid) + (1 | sentence_id))

    anova(phrase_m0, phrase_m1, phrase_m2)
```

# Perceptual Integration
## Load Data
```{r load-integration-data, echo = FALSE}
    integration_df <- load_eval_files("_integration_", create_correct_column = FALSE)
    integration_df <- merge_eval_files_with_training(integration_df, traing_word_exposures)

    integration_df <- within(integration_df, {
        integrated <- 1 - as.integer(mapply(grepl, pattern = selection, x = stimulus))
        auditory_stim <- as.factor(substring(stimulus, 1, 2))
        tactile_stim <- as.factor(substring(stimulus, 4, 5))
        speaker <- as.integer(substring(audibleFile, 8, 9))
    })

    # filter out bad speakers
    #integration_df <- dplyr::filter(integration_df, speaker > 3 & speaker != 7)
```

## Explore & Visualize ðŸ“ˆ
```{r plot-integration-data, echo = FALSE}
    integration_summary <- dplyr::summarise(
        dplyr::group_by(integration_df, pid, condition),
        correctPortion = 100 * mean(integrated),
        se = 100 * sd(integrated) / sqrt(length(integrated)),
        training_word_exposures = mean(training_word_exposures),
    )

    ggplot2::ggplot(integration_summary, ggplot2::aes(
        x = training_word_exposures, y = correctPortion
    )) +
        ggplot2::facet_wrap(facets = ggplot2::vars(pid)) +
        ggplot2::geom_hline(yintercept = 0, linetype = "dashed", color = "gray") +
        ggplot2::geom_line() +
        ggplot2::geom_point(size = 4) +
        ggplot2::geom_errorbar(
            ggplot2::aes(ymin = correctPortion - se, ymax = correctPortion + se),
            width = 250
        ) +
        ggplot2::ylim(0, 100) +
        ggplot2::labs(
            title = "Perceptual Integration",
            x = "Training Word Exposures",
            y = "Percentage Integrated"
        ) +
        ggplot2::theme_classic(base_size = 10)

    save_last_plot("integration.png")
```

## Model & Test
```{r model-integration-data, echo = FALSE}
    integration_model <- function(formula) bin_lme(formula = formula, data = integration_df)

    integration_m0 <- integration_model(integrated ~ 1 + (training_word_exposures | pid))
    integration_m1 <- integration_model(integrated ~ training_word_exposures + (training_word_exposures | pid))
    integration_m2 <- integration_model(
        integrated ~ training_word_exposures + (training_word_exposures | pid) + (1 | stimulus)
    )

    anova(integration_m0, integration_m1, integration_m2)
```
